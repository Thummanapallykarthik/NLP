{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLSvJ5maeeU6T3r2uvy64g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thummanapallykarthik/NLP/blob/main/assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eqs56OlqkheY",
        "outputId": "92103200-9fe6-4012-961f-6ba50b1ed093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "nltk.download('punkt') # Download the punkt tokenizer data\n",
        "nltk.download('punkt_tab') # Download the punkt_tab resource as suggested by the error\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('arxiv_data.csv', engine='python', nrows=1000)\n",
        "\n"
      ],
      "metadata": {
        "id": "si9XzaP_q8-T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('arxiv_data.csv')\n",
        "\n",
        "# Define the preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Removal of URLs (http, https, www patterns)\n",
        "    text = re.sub(r'http\\S+|https\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Removal of HTML tags (<...*>)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Removal of social media mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Removal of hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Conversion of all text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Removal of emojis using a comprehensive regex pattern\n",
        "    # This covers common emoji ranges\n",
        "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "    # Removal of any remaining special characters (keeping only alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    # Normalization of whitespace (reducing multiple spaces to single spaces and stripping leading/trailing spaces)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the function\n",
        "df['processed_summaries'] = df['summaries'].apply(preprocess_text)\n",
        "\n",
        "# Inspect the first few rows\n",
        "print(df[['summaries', 'processed_summaries']].head())\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('processed_arxiv_data.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynWzhArOrG0J",
        "outputId": "f08f08dd-2772-45e7-c887-343f77c44d10"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "\n",
            "                                 processed_summaries  \n",
            "0  stereo matching is one of the widely used tech...  \n",
            "1  the recent advancements in artificial intellig...  \n",
            "2  in this paper we proposed a novel mutual consi...  \n",
            "3  consistency training has proven to be an advan...  \n",
            "4  to ensure safety in automated driving the corr...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "# Load the previously processed data\n",
        "df = pd.read_csv('processed_arxiv_data.csv')\n",
        "\n",
        "# Download the necessary NLTK resource\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Recent NLTK versions/environments might require punkt_tab specifically if using newer versions\n",
        "try:\n",
        "    nltk.download('punkt_tab')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Tokenization using word_tokenize\n",
        "def tokenize_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "df['tokenized_summaries'] = df['processed_summaries'].apply(tokenize_text)\n",
        "\n",
        "# Inspect the results\n",
        "print(df[['processed_summaries', 'tokenized_summaries']].head())\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('tokenized_arxiv_data.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkOnR3nPs3M7",
        "outputId": "53b9a38b-794a-4e04-ee6f-719490b788f2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 processed_summaries  \\\n",
            "0  stereo matching is one of the widely used tech...   \n",
            "1  the recent advancements in artificial intellig...   \n",
            "2  in this paper we proposed a novel mutual consi...   \n",
            "3  consistency training has proven to be an advan...   \n",
            "4  to ensure safety in automated driving the corr...   \n",
            "\n",
            "                                 tokenized_summaries  \n",
            "0  [stereo, matching, is, one, of, the, widely, u...  \n",
            "1  [the, recent, advancements, in, artificial, in...  \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...  \n",
            "3  [consistency, training, has, proven, to, be, a...  \n",
            "4  [to, ensure, safety, in, automated, driving, t...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure the stopwords resource is available\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define the set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from tokenized_summaries\n",
        "df['filtered_summaries'] = df['tokenized_summaries'].apply(\n",
        "    lambda tokens: [w for w in tokens if w.lower() not in stop_words]\n",
        ")\n",
        "\n",
        "# Preview results\n",
        "print(df[['tokenized_summaries', 'filtered_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3GqjjUctkBH",
        "outputId": "c36d6e18-7a38-4833-9360-ae8793a56a84"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 tokenized_summaries  \\\n",
            "0  [stereo, matching, is, one, of, the, widely, u...   \n",
            "1  [the, recent, advancements, in, artificial, in...   \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...   \n",
            "3  [consistency, training, has, proven, to, be, a...   \n",
            "4  [to, ensure, safety, in, automated, driving, t...   \n",
            "\n",
            "                                  filtered_summaries  \n",
            "0  [stereo, matching, one, widely, used, techniqu...  \n",
            "1  [recent, advancements, artificial, intelligenc...  \n",
            "2  [paper, proposed, novel, mutual, consistency, ...  \n",
            "3  [consistency, training, proven, advanced, semi...  \n",
            "4  [ensure, safety, automated, driving, correct, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ensure the WordNet corpus is available\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # optional, improves lemmatization coverage\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply lemmatization to each token list\n",
        "df['lemmatized_summaries'] = df['filtered_summaries'].apply(\n",
        "    lambda tokens: [lemmatizer.lemmatize(w) for w in tokens]\n",
        ")\n",
        "\n",
        "# Preview results\n",
        "print(df[['filtered_summaries', 'lemmatized_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kPe0ZEBwCVt",
        "outputId": "56d8a343-1b74-425f-d3d8-fba8a72f3b37"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  filtered_summaries  \\\n",
            "0  [stereo, matching, one, widely, used, techniqu...   \n",
            "1  [recent, advancements, artificial, intelligenc...   \n",
            "2  [paper, proposed, novel, mutual, consistency, ...   \n",
            "3  [consistency, training, proven, advanced, semi...   \n",
            "4  [ensure, safety, automated, driving, correct, ...   \n",
            "\n",
            "                                lemmatized_summaries  \n",
            "0  [stereo, matching, one, widely, used, techniqu...  \n",
            "1  [recent, advancement, artificial, intelligence...  \n",
            "2  [paper, proposed, novel, mutual, consistency, ...  \n",
            "3  [consistency, training, proven, advanced, semi...  \n",
            "4  [ensure, safety, automated, driving, correct, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rejoin lemmatized words into a single string\n",
        "df['clean_summaries'] = df['lemmatized_summaries'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "# Preview results\n",
        "print(df[['lemmatized_summaries', 'clean_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgNegdIpwlqk",
        "outputId": "537b407a-3494-4aa4-be6f-ff9fd1fc69e5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                lemmatized_summaries  \\\n",
            "0  [stereo, matching, one, widely, used, techniqu...   \n",
            "1  [recent, advancement, artificial, intelligence...   \n",
            "2  [paper, proposed, novel, mutual, consistency, ...   \n",
            "3  [consistency, training, proven, advanced, semi...   \n",
            "4  [ensure, safety, automated, driving, correct, ...   \n",
            "\n",
            "                                     clean_summaries  \n",
            "0  stereo matching one widely used technique infe...  \n",
            "1  recent advancement artificial intelligence ai ...  \n",
            "2  paper proposed novel mutual consistency networ...  \n",
            "3  consistency training proven advanced semisuper...  \n",
            "4  ensure safety automated driving correct percep...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ensure necessary NLTK resources are available\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')   # required in newer NLTK versions\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Step 1: Define preprocessing function (regex cleaning)\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remove hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Remove special characters (keep alphanumeric + spaces)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Step 2: Unified NLTK preprocessing pipeline\n",
        "def nltk_preprocessing_pipeline(text):\n",
        "    # Regex cleaning\n",
        "    cleaned = preprocess_text(text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(cleaned)\n",
        "\n",
        "    # Stopword removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
        "\n",
        "    # Rejoin words\n",
        "    final_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return final_text\n",
        "\n",
        "# Step 3: Apply pipeline to dataset\n",
        "df = pd.read_csv('arxiv_data.csv', engine='python', nrows=1000)\n",
        "df['clean_summaries_pipeline'] = df['summaries'].apply(nltk_preprocessing_pipeline)\n",
        "\n",
        "# Preview results\n",
        "print(df[['summaries', 'clean_summaries_pipeline']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu38bi4wxaEv",
        "outputId": "0beee972-ef27-4641-8959-f0f0575c7bb6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "\n",
            "                            clean_summaries_pipeline  \n",
            "0  stereo matching one widely used technique infe...  \n",
            "1  recent advancement artificial intelligence ai ...  \n",
            "2  paper proposed novel mutual consistency networ...  \n",
            "3  consistency training proven advanced semisuper...  \n",
            "4  ensure safety automated driving correct percep...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(df['clean_summaries_pipeline'][0])\n",
        "nouns = []\n",
        "verbs = []\n",
        "for token in doc:\n",
        " if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
        "  nouns.append(token.text)\n",
        " elif token.pos_ == \"VERB\":\n",
        "  verbs.append(token.text)\n",
        "noun_freq = Counter(nouns)\n",
        "verb_freq = Counter(verbs)\n",
        "print(\"Noun Frequency:\", noun_freq)\n",
        "print(\"Verb Frequency:\", verb_freq)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N944Zmbwxo8Z",
        "outputId": "7d14f411-b666-4b4d-c5c8-2bfde2040270"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun Frequency: Counter({'stereo': 5, 'matching': 3, 'image': 2, 'speed': 2, 'application': 2, 'segmentation': 2, 'network': 2, 'term': 2, 'technique': 1, 'depth': 1, 'topic': 1, 'research': 1, 'find': 1, 'navigation': 1, '3d': 1, 'reconstruction': 1, 'field': 1, 'correspondence': 1, 'area': 1, 'challenge': 1, 'development': 1, 'cue': 1, 'result': 1, 'architecture': 1, 'leverage': 1, 'advantage': 1, 'paper': 1, 'aim': 1, 'comparison': 1, 'state': 1, 'art': 1, 'accuracy': 1, 'importance': 1, 'realtime': 1})\n",
            "Verb Frequency: Counter({'used': 2, 'matching': 1, 'inferring': 1, 'owing': 1, 'become': 1, 'driving': 1, 'finding': 1, 'nontextured': 1, 'shown': 1, 'improve': 1, 'proposed': 1, 'give': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure resources are available\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Reload dataset safely\n",
        "df = pd.read_csv('arxiv_data.csv', engine='python', nrows=1000)\n",
        "\n",
        "# Step 1: Tokenize\n",
        "df['tokenized_summaries'] = df['summaries'].apply(lambda x: word_tokenize(str(x)))\n",
        "\n",
        "# Step 2: POS tagging\n",
        "df['pos_summaries'] = df['tokenized_summaries'].apply(lambda tokens: pos_tag(tokens))\n",
        "\n",
        "# Step 3: NER\n",
        "def extract_entities(pos_tags):\n",
        "    chunked = ne_chunk(pos_tags)\n",
        "    entities = []\n",
        "    for subtree in chunked:\n",
        "        if hasattr(subtree, 'label'):\n",
        "            entity = \" \".join([token for token, pos in subtree.leaves()])\n",
        "            entities.append((entity, subtree.label()))\n",
        "    return entities\n",
        "\n",
        "# Apply to a small sample first to test\n",
        "df_sample = df.head(10).copy()\n",
        "df_sample['named_entities'] = df_sample['pos_summaries'].apply(extract_entities)\n",
        "\n",
        "print(\"Sample named entities:\")\n",
        "print(df_sample[['summaries', 'named_entities']])\n",
        "\n",
        "# Step 4: Frequency analysis (on sample for speed)\n",
        "all_entities = []\n",
        "for ents in df_sample['named_entities']:\n",
        "    all_entities.extend([entity for entity, label in ents])\n",
        "\n",
        "entity_freq = Counter(all_entities)\n",
        "print(\"\\nTop 10 most frequent named entities in sample:\")\n",
        "print(entity_freq.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOX1gCJc2N3q",
        "outputId": "c9480408-456d-4222-9f05-144980e43eb0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample named entities:\n",
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "5  High-quality training data play a key role in ...   \n",
            "6  Semantic segmentation of fine-resolution urban...   \n",
            "7  To mitigate the radiologist's workload, comput...   \n",
            "8  Generalising deep models to new data from new ...   \n",
            "9  The success of deep learning methods in medica...   \n",
            "\n",
            "                                      named_entities  \n",
            "0                                    [(Stereo, GPE)]  \n",
            "1  [(AI, ORGANIZATION), (AI, ORGANIZATION), (Euro...  \n",
            "2                                                 []  \n",
            "3  [(Consistency, GSP), (Atrial Segmentation, ORG...  \n",
            "4  [(Gaussian Mixture Models, PERSON), (GMM, ORGA...  \n",
            "5                         [(EdgeFlow, ORGANIZATION)]  \n",
            "6  [(Semantic, GPE), (CNNs, ORGANIZATION), (Visio...  \n",
            "7                      [(AutoEncoder, ORGANIZATION)]  \n",
            "8                                  [(Hence, PERSON)]  \n",
            "9                                                 []  \n",
            "\n",
            "Top 10 most frequent named entities in sample:\n",
            "[('EHT', 4), ('Mask', 3), ('AI', 2), ('GMM', 2), ('Morphological Snakes', 2), ('CNNs', 2), ('Stereo', 1), ('European', 1), ('Health', 1), ('Robustness', 1)]\n"
          ]
        }
      ]
    }
  ]
}